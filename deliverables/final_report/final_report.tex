\documentclass[]{aiaa-tc}% insert '[draft]' option to show overfull boxes

 \usepackage{varioref}%  smart page, figure, table, and equation referencing
 \usepackage{wrapfig}%   wrap figures/tables in text (i.e., Di Vinci style)
 \usepackage{threeparttable}% tables with footnotes
 \usepackage{dcolumn}%   decimal-aligned tabular math columns
  \newcolumntype{d}{D{.}{.}{-1}}
 \usepackage{nomencl}%   nomenclature generation via makeindex
  \makeglossary

  \usepackage{graphicx}
  \usepackage{caption}
  \usepackage{subcaption}
%  \usepackage{listings}
  
% \usepackage{subfigure}% subcaptions for subfigures
% \usepackage{subfigmat}% matrices of similar subfigures, aka small mulitples
 \usepackage{fancyvrb}%  extended verbatim environments
  \fvset{fontsize=\footnotesize,xleftmargin=2em}
 \usepackage{lettrine}%  dropped capital letter at beginning of paragraph
 \usepackage[export]{adjustbox}
 \usepackage{url}

 \title{GPU acceleration of wall distance calculation for
   computational fluid dynamics codes}
 \author{
   Nathan A. Wukie\thanks{Graduate student researcher, School of Aerospace Systems, ML 70, 
     Cincinnati, OH, AIAA Student Member.},
   \ Vasanth Ganapathy\thanks{Student, University of Cincinnati},
   \ Chris Park\thanks{Student, University of Cincinnati} \\
   {\normalsize\itshape
     University of Cincinnati, Cincinnati, OH, 45221, USA}\\
 }

 % Data used by 'handcarry' option
 %\AIAApapernumber{YEAR-NUMBER}
 %\AIAAconference{Conference Name, Date, and Location}
 %\AIAAcopyright{\AIAAcopyrightD{YEAR}}

 % Define commands to assure consistent treatment throughout document
 \newcommand{\eqnref}[1]{(\ref{#1})}
 \newcommand{\class}[1]{\texttt{#1}}
 \newcommand{\package}[1]{\texttt{#1}}
 \newcommand{\file}[1]{\texttt{#1}}
 \newcommand{\BibTeX}{\textsc{Bib}\TeX}

% Add highlighting - Wukie
\usepackage{color,soul}

\begin{document}

\maketitle

%\begin{abstract}
%Insert abstract
%\end{abstract}

%\section*{Nomenclature}

%\begin{tabbing}
%  XXX \= \kill% this line sets tab stop
%  $\rho$ \> Density \\
%  $\delta(f)$ \> Dirac delta function \\
%  $H(f)$ \> Heaviside function\\
%  $T_{ij}$ \> Lighthill stress tensor \\
%  $P_{ij}$ \> Compressive stress tensor \\
%  $u$ \> X-velocity \\
%  $v$ \> Y-velocity \\
%  $p'$ \> Pressure fluctuation\\
%  $x$ \> Tensor coordinate\\
%  $t$  \> Time\\
%  $c_o$ \> Reference speed of sound\\[5pt]
%  \textit{Subscript}\\
%  $i,j$ \> Tensor indices \\
%  $n$ \> Normal component\\
% \end{tabbing}

\section{Introduction}
\lettrine[nindent=0pt]{C}{omputational} fluid dynamics(CFD) is a branch of
simulation sciences that focuses on predicting fluid flows based on a
set of governing physical equations. There are many different methods
for accomplishing this that span both, different sets of governing
equations, in addition to different numerical methods for computing a
solution. Some different sets of equations that can be solved include
the potential flow equation, streamfunction-vorticity equations, Euler
equations, and Navier-Stokes equations. Analytical solutions to these
sets of equations are often not available except in some extremely
simple cases. Interesting geometries require a discretized
approach. Some common discretization methods include Finite Difference,
Finite Volume, and Finite Element methods. 

This research effort is focused on a problem for a subset
of those methods. In particular, we are interested in efficient wall
distance calculation algorithms to support Reynolds-Averaged
Navier-Stokes(RANS) solvers using a cell-centered, Finite Volume discretization with a
moving mesh capability.

\subsection{Background}

\subsubsection{Reynolds-Averaged Navier-Stokes Equations}
The Reynolds-Averaged Navier-Stokes(RANS) equations govern viscous, laminar
and turbulent flows. For turbulent flows, the turbulence is accounted
for via a turbulence model, which usually takes the
form of an additional partial differential equation or sometimes a set
of partial differential equations that solve for turbulent working
variables.

One general method for formulating these turbulence models is to
include ``production'' and ``destruction'' terms for the turbulent
working variables. In this way, phenomena that typically increase
turbulence, such as vorticity, will ``produce'' a turbulence effect by
increasing the turbulent working variable. One variable that governs such
turbulence production/destruction terms is the distance of a grid cell to
the nearest solid wall; simply known as the wall distance, $d_{wall}$.\cite{allmaras:12}


\subsubsection{Cell-centered, Finite Volume discretization}
The numerical method that this investigation directly supports is a
cell-centered, finite volume discretization, although it could be
readily extended to other methods with some modifications. The finite
volume method relies on an integral form of the governing equations,
and the method we are interested in stores all variables at the center
of each computational cell. The values in each cell can then be
updated, based on boundary fluxes computed at the interface of each
computational cell.\cite{hirsch:94} The value of interest here is the $d_{wall}$
variable required by the turbulence model.

\subsubsection{Moving mesh calculation}
Some investigations that use fluid simulations are interested in moving
body problems such as store separation for aircraft, and rotating
machinery, among others. One way to accomplish this is to use
body-local grids that overlap a background grid as shown in Figure~\ref{f:chimera_grid}.\cite{galbraith:13phd} In this way, the body
grid can move on-top of the background mesh to facilitate body motion. This
usually works by computing an iteration for the CFD code, computing
the force on the body of interest, computing the distance moved, and
updating coordinates of the body-local grid.

\begin{figure}
  \centering
  \includegraphics[width=0.3\linewidth]{figures/grid/chimera_grid}
  \caption{Chimera, overlapping grid for moving airfoil simulation}
  \label{f:chimera_grid}
\end{figure}


\subsubsection{Motivation for efficient wall distance computation}
Traditionally, the wall distance calculation would be of little
importance from an efficiency perspective. In stationary grid
simulations, the wall distance does not change, and so it would only
be performed once as a pre-processing step, which would represent a
trivial amount of time compared to the overall calculation. For moving
body simulations however, the wall distance can change after each
update of the body position and thus has to be recomputed for every
iteration. It becomes extremely important then, to have an efficient
method for recomputing the wall distance in order to maintain
reasonable computation times. Using graphics processing units(GPU's)
to accelerate the wall distance computation is seen as a promising
method for improving computational efficiency for moving body
problems.

\section{Design and optimization approach}
The application is laid out as a main function that consists of a
grid preprocessor, and then function calls to the respective wall
distance calculation routines. The preprocessor reads the
computational grid and stores the cell centers as two arrays,
$x$-coordinates and $y$-coordinates. It also reads in geometry faces and
stores their respective $x$ and $y$ coordinates in two separate
arrays. These are the input data to the wall distance calculation
routines. There are two wall distance calculation methods that are
being used for this investigation. They are a 'Brute Force' method and
'Advancing Boundary' method and will be described in the subsequent
sections. A diagram of the main application can be seen in Figure~\ref{f:main_application}.

\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{figures/main_application_2}
  \caption{Main application diagram}
  \label{f:main_application}
\end{figure}


\subsection{Brute force method}

\subsubsection{Complexity}

\subsubsection{Parallelization}


\subsection{Advancing boundary method}
The ``Advancing Boundary'' method used in this investigation is based
on work done by Roget and Sitaraman, where they used
voxelized marching spheres to reduce the wall distance calculation to a small
subset of the original calculation.\cite{roget:12} The following process is largely the
same as the description in the paper, with modifications to auxiliary
cell structures and elimination of some of the more nuanced features.

The advancing boundary method begins with a preprocessing routine that
computes a set of auxiliary cells, which are then provided to the wall
distance calculation routine. This is shown
as a diagram in Figure~\ref{f:ab_preprocessor}.

\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{figures/preprocessor/preprocessor_diagram}
  \caption{Advancing Boundary - Preprocessor}
  \label{f:ab_preprocessor}
\end{figure}


The goal of the auxiliary cells is that each would contain several
solid geometry faces. In this way, the wall distance routine can first
search through the smaller number of auxiliary cells to determine
those eligible for containing the closest solid face. This allows the
algorithm to reduce the number of total faces that need checked by
eliminating multiple solid faces each time an auxiliary cell is
eliminated. This wall distance calculation process is laid out in the diagram
in Figure~\ref{f:ab_diagram}.


\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{figures/auxiliary_grid/algorithm_diagram}
  \caption{Advancing Boundary - Wall distance calculation}
  \label{f:ab_diagram}
\end{figure}



\subsubsection{Complexity}

The work complexity for this algorithm is related to the number of
cells in the computational grid and the number of solid geometry faces
that each cell has to search. In the worst case, each computational
grid must search through each solid face to find the closest
face. Therefore, for a case with $n$ computational cells and $m$ solid
geometry faces, the work complexity of the algorithm is O($n \times
m$). However, the computation for each cell executes independent of
all the other cells and requires only one step, which is to search
through all of the solid geometry faces. As a result, the step
complexity of the algorithm is O($1$).

\subsubsection{Parallelization}
This algorithm, much in the same way as the brute force method, has a
lot of parallelism that can be exploited for improved
performance. Specifically, the wall distance calculation for each
computational cell executes independently of all other computational
cells. The parallel approach used for this study is a thread-per-cell
technique, where one wall distance kernel thread is launched for each
computational cell.


\begin{itemize}
\item Work: O($n \times m$)
\item Step: O(1)
\end{itemize}


\section{Application performance and results}
The following sections present the methods used to benchmark each
algorithm, along with the various modifications implemented to improve
performance. Finally, timings for both serial and parallel
implementations are presented for both methods to evaluate the overall
speedup achieved via the parallel GPU implementation.


\subsubsection{Problem set}
The problem set chosen for this investigation was a solid circular
body, surrounded by a circular discretized domain. This simple
geometry allowed a computational grid to be easily refined in order to
load algorithms more heavily and reduce the effect of computational
overhead on the overall timings. The grid and wall distance field can
be seen in Figure~\ref{f:circle}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{figures/grid/circle_grid}
  \caption{Computational grid and corresponding wall distance field}
  \label{f:circle}
\end{figure}


\subsubsection{Brute-force algorithm}




\subsubsection{Advancing boundary algorithm}
Several implementations of the advancing boundary algorithm were
tested, both for serial and parallel algorithms. The different methods
for improving performance included memory management and changing storage
structures. The initial serial algorithm was implemented with an
'Auxiliary Cell' struct that would include the geometric information
for a given auxiliary cell, in addition to providing some method for
accessing the faces that were contained in the face. The general
layout of this struct was similar for all iterations of the serial and
parallel codes. Only the face storage mechanism was altered for more
efficient memory accesses. The first iteration of these stored the
faces as a linked-list of face structs, each of which stored the x,y
pair associated with a face center. This initial configuration is shown in
Figure~\ref{f:initial_aux_cell}.

\begin{figure}
  \centering
  \includegraphics[trim=11.2cm 21.8cm 24cm 5.2cm,
  clip=true, width=0.3\linewidth]{figures/cells/cell_linked_list}
  \caption{Initial auxiliary cell struct}
  \label{f:initial_aux_cell}
\end{figure}

Other methods for storing the faces included pointers to face arrays,
explicitly allocated arrays in the cell struct, and also an array of
face indices that would facilitate access to the correct face in an
external array.

\begin{figure}
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[trim=11.2cm 21.8cm 24cm 4.8cm,
    clip=true, width=0.7\linewidth]{figures/cells/cell_pointers}
    \caption{Struct A: Pointers to storage arrays}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[trim=11.2cm 21.8cm 24cm 4.8cm,
    clip=true, width=0.7\linewidth]{figures/cells/cell_array}
    \caption{Struct B: Explicitly allocated in-cell storage}
  \end{subfigure}
  
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[trim=11.2cm 21.8cm 24cm 4.8cm,
    clip=true, width=0.7\linewidth]{figures/cells/cell_arrays}
    \caption{Struct C: Separate in-cell storage}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[trim=11.2cm 21.8cm 24cm 4.8cm,
    clip=true, width=0.7\linewidth]{figures/cells/cell_index}
    \caption{Struct D: Face index array}
  \end{subfigure}
  
  \caption{Auxiliary cell struct versions}
\end{figure}


The timings for the Serial algorithm are shown in
Table~\ref{t:serial}. The linked-list method of storing faces was the
most efficient access method of those tested. The best-performing
serial algorithm was then compared to the parallel algorithms to gauge
the parallel speedup factor. The parallel timings are shown in
Table~\ref{t:parallel}. The initial implementation used all global
memory accesses and subsequent improvements focused on bringing in
reusable data into shared memory or local registers. Some final
performance improvements were achieved by restructuring the face
storage structure.

\begin{table}[h]
  \centering
  \begin{tabular}{ccc}
    Iteration & Description & Average Time (ms) \\
    \hline\hline
    T1 & Baseline, linked-list of faces & 8796 \\
    T2 & New aux cell struct & 11044 \\
    T3 & New aux cell struct & 10007 \\
    \hline
  \end{tabular}
  \caption{Serial ``Advancing Boundary'' algorithm development and performance}
  \label{t:serial}
\end{table}



\begin{table}[h]
  \centering
  \begin{tabular}{cccc}
    Iteration & Description & Average Time (ms) & Speedup vs. Serial\\
    \hline\hline
    T1 & Baseline, no shared memory & 134 & 65\\
    T2 & Memory management & 114 & 77\\
    T3 & Memory management & 114 & 77\\
    T4 & Memory management & 112 & 78\\
    T5 & Memory management & 113 & 78\\
    T4 - new cell - 1 & New aux cell struct & 106 & 83\\
    T4 - new cell - 2& New aux cell struct & 102 & 86\\
    \hline
  \end{tabular}
  \caption{Parallel ``Advancing Boundary'' algorithm development and performance}
  \label{t:parallel}
\end{table}



\section{Schedule and division of work}
Figure~\ref{f:schedule} shows the work split between the team contributors.

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/schedule_update}
  \caption{Development schedule and division of work}
  \label{f:schedule}
\end{figure}


\subsection{Self-assessment: Nathan Wukie}
I provided the primary topic and guidance for the project, along with
the application framework, preprocessor, and cmake
usage. Additionally, I was responsible for the ``Advancing Boundary''
algorithm development, implementation, and parallelization.

\subsection{Self-assessment: Vasanth Ganapathy}


% produces the bibliography section when processed by BibTeX
\nocite{*}
\bibliography{bibliography}
\bibliographystyle{aiaa}

\end{document}

% - Release $Name:  $ -
